# -*- coding: utf-8 -*-
"""Sentimen Analysis Chat GPT

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vDiReP7GP8rRUbzYV3kTaMV_1WJHNuRd

# Install Library
"""

!pip install google_play_scraper

from google_play_scraper import app, reviews, Sort, reviews_all

import pandas as pd
pd.options.mode.chained_assignment = None
import numpy as np
seed = 0
np.random.seed(seed)
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score

import datetime as dt
import re
import string
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

!pip install sastrawi
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory

from wordcloud import WordCloud

import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('punkt_tab')

"""# Scraping Dataset"""

# Mengimpor pustaka google_play_scraper untuk mengakses ulasan dan informasi aplikasi dari Google Play Store.
from google_play_scraper import app, reviews_all, Sort

# Mengambil semua ulasan dari aplikasi dengan ID 'com.mobile.legend' di Google Play Store.
scrapreview = reviews_all(
    'com.openai.chatgpt',
    lang='id',
    country='id',
    sort=Sort.MOST_RELEVANT,
    count=30000
)

# Menyimpan ulasan dalam file CSV
import csv

with open('ulasan_aplikasi.csv', mode='w', newline='', encoding='utf-8') as file:
  writer = csv.writer(file)
  writer.writerow(['Review'])
  for review in scrapreview:
    writer.writerow([review['content']]) # menulis konten ulasan ke dalam file csv

"""# Load the dataset"""

gpt_reviews_df = pd.DataFrame(scrapreview)
gpt_reviews_df.shape
gpt_reviews_df.head()
gpt_reviews_df.to_csv('ulasan_aplikasi.csv', index=False)

# Membuat DataFrame dari hasil scrapreview
gpt_reviews_df = pd.DataFrame(scrapreview)

# Menghitung jumlah baris dan kolom dalam DataFrame
jumlah_ulasan, jumlah_kolom = gpt_reviews_df.shape

gpt_reviews_df

gpt_reviews_df.info()

clean_df = gpt_reviews_df.drop_duplicates()
clean_df.info()

jumlah_ulasan, jumlah_kolom = gpt_reviews_df.shape
print(f'Jumlah ulasan: {jumlah_ulasan}')
print(f'Jumlah kolom: {jumlah_kolom}')

clean_df = gpt_reviews_df[['content']]
clean_df = clean_df.head(30000)
clean_df

"""# Preprocessing Text"""

import re
import string
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
import emoji

def cleaningText(text):
    text = re.sub(r'@[A-Za-z0-9]+', '', text) # menghapus mention
    text = re.sub(r'#[A-Za-z0-9]+', '', text) # menghapus hashtag
    text = re.sub(r'RT[\s]', '', text) # menghapus RT
    text = re.sub(r"http\S+", '', text) # menghapus link
    text = re.sub(r'[0-9]+', '', text) # menghapus angka
    text = re.sub(r'[^\w\s]', '', text) # menghapus karakter selain huruf dan angka

    text = text.replace('\n', ' ') # mengganti baris baru dengan spasi
    text = text.translate(str.maketrans('', '', string.punctuation)) # menghapus semua tanda baca
    text = text.strip(' ') # menghapus karakter spasi dari kiri dan kanan teks
    return text

def casefoldingText(text): # Mengubah semua karakter dalam teks menjadi huruf kecil
    text = text.lower()
    return text

def tokenizingText(text): # Memecah atau membagi string, teks menjadi daftar token
    text = word_tokenize(text)
    return text

def filteringText(text): # Menghapus stopwords dalam teks
    listStopwords = set(stopwords.words('indonesian'))
    listStopwords1 = set(stopwords.words('english'))
    listStopwords.update(listStopwords1)
    listStopwords.update(['iya','yaa','gak','nya','na','sih','ku',"di","ga","ya","gaa","loh","kah","woi","woii","woy"])
    filtered = []
    for txt in text:
        if txt not in listStopwords:
            filtered.append(txt)
    text = filtered
    return text

def stemmingText(text): # Mengurangi kata ke bentuk dasarnya yang menghilangkan imbuhan awalan dan akhiran atau ke akar kata
    # Membuat objek stemmer
    factory = StemmerFactory()
    stemmer = factory.create_stemmer()

    # Memecah teks menjadi daftar kata
    words = text.split()

    # Menerapkan stemming pada setiap kata dalam daftar
    stemmed_words = [stemmer.stem(word) for word in words]

    # Menggabungkan kata-kata yang telah distem
    stemmed_text = ' '.join(stemmed_words)

    return stemmed_text

def toSentence(list_words): # Mengubah daftar kata menjadi kalimat
    sentence = ' '.join(word for word in list_words)
    return sentence

slangwords = {"@": "di", "abis": "habis", "wtb": "beli", "masi": "masih", "wts": "jual", "wtt": "tukar", "bgt": "banget", "maks": "maksimal" "â€¦"}
def fix_slangwords(text):
    words = text.split()
    fixed_words = []

    for word in words:
        if word.lower() in slangwords:
            fixed_words.append(slangwords[word.lower()])
        else:
            fixed_words.append(word)

    fixed_text = ' '.join(fixed_words)
    return fixed_text

# Membersihkan teks dan menyimpannya di kolom 'text_clean'
clean_df['text_clean'] = clean_df['content'].apply(cleaningText)

# Mengubah huruf dalam teks menjadi huruf kecil dan menyimpannya di 'text_casefoldingText'
clean_df['text_casefoldingText'] = clean_df['text_clean'].apply(casefoldingText)

# Mengganti kata-kata slang dengan kata-kata standar dan menyimpannya di 'text_slangwords'
clean_df['text_slangwords'] = clean_df['text_casefoldingText'].apply(fix_slangwords)

# Memecah teks menjadi token (kata-kata) dan menyimpannya di 'text_tokenizingText'
clean_df['text_tokenizingText'] = clean_df['text_slangwords'].apply(tokenizingText)

# Menghapus kata-kata stop (kata-kata umum) dan menyimpannya di 'text_stopword'
clean_df['text_stopword'] = clean_df['text_tokenizingText'].apply(filteringText)

# Menggabungkan token-token menjadi kalimat dan menyimpannya di 'text_akhir'
clean_df['text_akhir'] = clean_df['text_stopword'].apply(toSentence)

clean_df

"""# Pelabelan Data"""

clean_df = clean_df.dropna()
print("Jumlah Ulasan", clean_df.shape[0])

import csv
import requests
from io import StringIO

lexicon_positive = dict()

response = requests.get('https://raw.githubusercontent.com/angelmetanosaa/dataset/main/lexicon_positive.csv')

if response.status_code == 200:
    reader = csv.reader(StringIO(response.text), delimiter=',')
    for row in reader:
        lexicon_positive[row[0]] = int(row[1])
else:
    print("Failed to fetch positive lexicon data")

lexicon_negative = dict()

response = requests.get('https://raw.githubusercontent.com/angelmetanosaa/dataset/main/lexicon_negative.csv')

if response.status_code == 200:
    reader = csv.reader(StringIO(response.text), delimiter=',')
    for row in reader:
        lexicon_negative[row[0]] = int(row[1])
else:
    print("Failed to fetch negative lexicon data")

def sentiment_analysis_lexicon_indonesia(text):
    score = 0  # Inisialisasi skor sentimen ke 0

    for word in text:
        if word in lexicon_positive:
            score += lexicon_positive[word]  # Tambahkan skor dari kata positif
        elif word in lexicon_negative:
            score += lexicon_negative[word]  # Kurangi skor dari kata negatif

    # Tentukan polaritas berdasarkan skor
    if score > 0:
        polarity = 'positive'
    elif score < 0:
        polarity = 'negative'
    else:
        polarity = 'neutral'  # Jika skor adalah 0, anggap sebagai netral

    return score, polarity  # Mengembalikan skor sentimen dan polaritas

results = clean_df['text_stopword'].apply(sentiment_analysis_lexicon_indonesia)
results = list(zip(*results))
clean_df['polarity_score'] = results[0]
clean_df['polarity'] = results[1]
print(clean_df['polarity'].value_counts())

clean_df.to_csv('scraping_data.csv', index=False)

"""# Data Splitting dan Ekstraksi Fitur dengan TF-IDF"""

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer

# Pisahkan data menjadi fitur (tweet) dan label (sentimen)
X = clean_df['text_akhir']
y = clean_df['polarity']

# Ekstraksi fitur dengan TF-IDF
tfidf = TfidfVectorizer(max_features=200, min_df=17, max_df=0.8 )
X_tfidf = tfidf.fit_transform(X)

# Konversi hasil ekstraksi fitur menjadi dataframe
features_df = pd.DataFrame(X_tfidf.toarray(), columns=tfidf.get_feature_names_out())

# Menampilkan hasil ekstraksi fitur
features_df

# Bagi data menjadi data latih dan data uji
X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)

"""# Modeling Data

## Random Forest
"""

from sklearn.ensemble import RandomForestClassifier

# Membuat objek model Random Forest
random_forest = RandomForestClassifier()

# Melatih model Random Forest pada data pelatihan
random_forest.fit(X_train.toarray(), y_train)

# Prediksi sentimen pada data pelatihan dan data uji
y_pred_train_rf = random_forest.predict(X_train.toarray())
y_pred_test_rf = random_forest.predict(X_test.toarray())

# Evaluasi akurasi model Random Forest
accuracy_train_rf = accuracy_score(y_pred_train_rf, y_train)
accuracy_test_rf = accuracy_score(y_pred_test_rf, y_test)

# Menampilkan akurasi
print('Random Forest - accuracy_train:', accuracy_train_rf)
print('Random Forest - accuracy_test:', accuracy_test_rf)

"""Percobaan"""

# List kalimat baru
list_kalimat = [
    "aplikasinya sangat bagus dan membantu",
    "Mantap banget",
    "aplikasinya jelek banget",
    "aplikasi gak jelas lemot",
    "aplikasinya bagus",
    "aplikasinya jelek",
    "jelek lemot sering keluar sendiri"
]

# Transformasi ke TF-IDF
transform_list = tfidf.transform(list_kalimat).toarray()

# Prediksi dengan model (contoh: Random Forest)
hasil_prediksi = random_forest.predict(transform_list)

# Tampilkan hasil prediksi
for kalimat, prediksi in zip(list_kalimat, hasil_prediksi):
    print(f"Kalimat: '{kalimat}' --> Sentimen: {prediksi}")

"""## Support Vector Machine"""

from sklearn.svm import SVC

svm = SVC(kernel='linear', C=10.0, cache_size=1024)

svm.fit(X_train.toarray(), y_train)

y_pred_train_svm = svm.predict(X_train.toarray())
y_pred_test_svm = svm.predict(X_test.toarray())

accuracy_train_svm = accuracy_score(y_pred_train_svm, y_train)
accuracy_test_svm = accuracy_score(y_pred_test_svm, y_test)

print("svm accuracy train:", accuracy_train_svm)
print("svm accuracy test:", accuracy_test_svm)

"""Percobaan"""

# List kalimat baru
list_kalimat = [
    "aplikasinya sangat bagus dan membantu",
    "Mantap banget",
    "aplikasinya jelek banget",
    "aplikasi gak jelas lemot",
    "aplikasinya bagus",
    "aplikasinya jelek",
    "jelek lemot sering keluar sendiri"
]

# Transformasi ke TF-IDF
transform_list = tfidf.transform(list_kalimat).toarray()

# Prediksi dengan model (contoh: Random Forest)
hasil_prediksi = random_forest.predict(transform_list)

# Tampilkan hasil prediksi
for kalimat, prediksi in zip(list_kalimat, hasil_prediksi):
    print(f"Kalimat: '{kalimat}' --> Sentimen: {prediksi}")

"""## Linear Regression"""

from sklearn.linear_model import LogisticRegression

# Membuat objek model Logistic Regression
logistic_regression = LogisticRegression()

# Melatih model Logistic Regression pada data pelatihan
logistic_regression.fit(X_train.toarray(), y_train)

# Prediksi sentimen pada data pelatihan dan data uji
y_pred_train_lr = logistic_regression.predict(X_train.toarray())
y_pred_test_lr = logistic_regression.predict(X_test.toarray())

# Evaluasi akurasi model Logistic Regression pada data pelatihan
accuracy_train_lr = accuracy_score(y_pred_train_lr, y_train)

# Evaluasi akurasi model Logistic Regression pada data uji
accuracy_test_lr = accuracy_score(y_pred_test_lr, y_test)

# Menampilkan akurasi
print('Logistic Regression - accuracy_train:', accuracy_train_lr)
print('Logistic Regression - accuracy_test:', accuracy_test_lr)

# List kalimat baru
list_kalimat = [
    "aplikasinya sangat bagus dan membantu",
    "Mantap banget",
    "aplikasinya jelek banget",
    "aplikasi gak jelas lemot",
    "aplikasinya bagus",
    "aplikasinya jelek",
    "jelek lemot sering keluar sendiri"
]

# Transformasi ke TF-IDF
transform_list = tfidf.transform(list_kalimat).toarray()

# Prediksi dengan model (contoh: Random Forest)
hasil_prediksi = logistic_regression.predict(transform_list)

# Tampilkan hasil prediksi
for kalimat, prediksi in zip(list_kalimat, hasil_prediksi):
    print(f"Kalimat: '{kalimat}' --> Sentimen: {prediksi}")